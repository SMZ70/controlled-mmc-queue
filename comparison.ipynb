{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This Jupyter Notebook implements a comparison of different reinforcement learning methods and a value iteration method for a given M/M/c queueing system. The implemented methods include:\n",
    "\n",
    "- Value iteration\n",
    "- Tabular Q-learning\n",
    "- SARSA\n",
    "- Deep Q-learning\n",
    "- A2C\n",
    "\n",
    "The performance of the resulting policy of the methods are compared using simulation. The system is simulated over several replications and the expected value of the number of items in the system is reported."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import required packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# import the algorithms from controlledmmcqueue package\n",
    "from controlledmmcqueue import MMCEnv, ValueIteration, TabularQLearner, SARSA, DQNLearner, A2CLearner\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build the M/M/c environment\n",
    "\n",
    "Before running the algorithms, the M/M/c environment needs to be built. The environment is based on the `gym.Env`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# queueing system parameters\n",
    "arrival_rate = 2.0\n",
    "service_rates = [2.0, 0.1]\n",
    "queue_capacity = 100\n",
    "\n",
    "# maximum events to use as the termination condition for each episode of the environment\n",
    "max_events_per_episode = 2000\n",
    "\n",
    "env = MMCEnv(arr_rate=arrival_rate,\n",
    "             srv_rates=service_rates, queue_cap=queue_capacity,\n",
    "             punish_reward=-99999,\n",
    "             scale_rewards=False,\n",
    "             max_events=max_events_per_episode)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Run the algorithms to learn the optimal Policy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Value iteration\n",
    "\n",
    "Value iteration is implemented to compute the optimal policy. The transition probabilities of the system are known to the algorithm. The result of value iteration can be used as a benchmark."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=   200, max_delta_v=  11.96379182\u001B[0m\n",
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=   400, max_delta_v=   1.46660913\u001B[0m\n",
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=   600, max_delta_v=   0.18060980\u001B[0m\n",
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=   800, max_delta_v=   0.02223499\u001B[0m\n",
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=  1000, max_delta_v=   0.00273364\u001B[0m\n",
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=  1200, max_delta_v=   0.00033582\u001B[0m\n",
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=  1400, max_delta_v=   0.00004127\u001B[0m\n",
      "learners   \u001B[0m[inf] [LEARN-ValueIteration]   iter=  1536, max_delta_v=   0.00000993\u001B[0m\n",
      "learners   \u001B[32m[suc] [LEARN-ValueIteration]   Learning complete\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the learner\n",
    "learner_val_iter = ValueIteration(mmc_env=env, discount_factor=0.99)\n",
    "\n",
    "# run the algorithm to learn the optimal policy\n",
    "learner_val_iter.learn(convergence_threshold=1e-5, log_frequency=200)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimal policy when the fast server is busy\n",
    "\n",
    "We know that the optimal policy for an M/M/2 system, in case the fast server is busy is of a threshold policy based on the number of items waiting in the queue. Let's check if it is the case in the optimal policy learned by value iteration."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for n_in_queue =   1: ()\n",
      "Optimal policy for n_in_queue =   2: ()\n",
      "Optimal policy for n_in_queue =   3: ()\n",
      "Optimal policy for n_in_queue =   4: ()\n",
      "Optimal policy for n_in_queue =   5: (2,)\n",
      "Optimal policy for n_in_queue =   6: (2,)\n",
      "Optimal policy for n_in_queue =   7: (2,)\n",
      "Optimal policy for n_in_queue =   8: (2,)\n",
      "Optimal policy for n_in_queue =   9: (2,)\n",
      "Optimal policy for n_in_queue =  10: (2,)\n",
      "Optimal policy for n_in_queue =  11: (2,)\n",
      "Optimal policy for n_in_queue =  12: (2,)\n",
      "Optimal policy for n_in_queue =  13: (2,)\n",
      "Optimal policy for n_in_queue =  14: (2,)\n",
      "Optimal policy for n_in_queue =  15: (2,)\n",
      "Optimal policy for n_in_queue =  16: (2,)\n",
      "Optimal policy for n_in_queue =  17: (2,)\n",
      "Optimal policy for n_in_queue =  18: (2,)\n",
      "Optimal policy for n_in_queue =  19: (2,)\n"
     ]
    }
   ],
   "source": [
    "fast_server = np.argmax(service_rates)\n",
    "slow_server = np.argmin(service_rates)\n",
    "\n",
    "for n_in_queue in range(1, queue_capacity // 5):\n",
    "    state = [n_in_queue] + [1 if s == fast_server else 0 for s in range(len(service_rates))]\n",
    "    print(f\"Optimal policy for n_in_queue = {n_in_queue:3d}: {learner_val_iter.predict(state)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen, the optimal policy is not to route to the slow server, unless more than 4 items are waiting in the queue."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimal policy when both servers are available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "fast_server = np.argmax(service_rates)\n",
    "slow_server = np.argmin(service_rates)\n",
    "\n",
    "for n_in_queue in range(1, queue_capacity // 5):\n",
    "    state = [n_in_queue] + [0 for s in range(len(service_rates))]\n",
    "    print(f\"Optimal policy for n_in_queue = {n_in_queue:3d}: {learner_val_iter.predict(state)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy for n_in_queue =   1: (1,)\n",
      "Optimal policy for n_in_queue =   2: (1,)\n",
      "Optimal policy for n_in_queue =   3: (1,)\n",
      "Optimal policy for n_in_queue =   4: (1,)\n",
      "Optimal policy for n_in_queue =   5: (1,)\n",
      "Optimal policy for n_in_queue =   6: (1, 2)\n",
      "Optimal policy for n_in_queue =   7: (1, 2)\n",
      "Optimal policy for n_in_queue =   8: (1, 2)\n",
      "Optimal policy for n_in_queue =   9: (1, 2)\n",
      "Optimal policy for n_in_queue =  10: (1, 2)\n",
      "Optimal policy for n_in_queue =  11: (1, 2)\n",
      "Optimal policy for n_in_queue =  12: (1, 2)\n",
      "Optimal policy for n_in_queue =  13: (1, 2)\n",
      "Optimal policy for n_in_queue =  14: (1, 2)\n",
      "Optimal policy for n_in_queue =  15: (1, 2)\n",
      "Optimal policy for n_in_queue =  16: (1, 2)\n",
      "Optimal policy for n_in_queue =  17: (1, 2)\n",
      "Optimal policy for n_in_queue =  18: (1, 2)\n",
      "Optimal policy for n_in_queue =  19: (1, 2)\n"
     ]
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}